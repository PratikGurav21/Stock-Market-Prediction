{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52db0111",
   "metadata": {},
   "source": [
    "# GRIP : The Spark Foundation\n",
    "\n",
    "# #GRIPNOV2022\n",
    "\n",
    "# Task 7 : Stock Market Prediction using Numerical and Textual Analysis\n",
    "\n",
    "# Problem Statement : To create a hybrid model for stock performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines.\n",
    "\n",
    "# Auther : Pratik Janaba Gurav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236823c",
   "metadata": {},
   "source": [
    "Historical stock prices :https://finance.yahoo.com/ Textual news headlines : https://bit.ly/36fFPI6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d8e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\LAPTECH\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#importing libraries \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Dense, Activation\n",
    "\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4cbb3e",
   "metadata": {},
   "source": [
    "### Importing the Numerical dataset and performing Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = pd.read_csv('/content/drive/MyDrive/TheSparkFoundation/AAPL.csv')\n",
    "stock_headlines = pd.read_csv('/content/drive/MyDrive/TheSparkFoundation/india-news-headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bedc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying stock price dataset\n",
    "stock_price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b9af5",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a98675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying number of records in both stock_price and stock_headlines datasets\n",
    "len(stock_price), len(stock_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2dd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values in both the datasets\n",
    "stock_price.isna().any(), stock_headlines.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762b43e",
   "metadata": {},
   "source": [
    "### Stock Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates\n",
    "stock_price = stock_price.drop_duplicates()\n",
    "\n",
    "# coverting the datatype of column 'Date' from type object to type 'datetime'\n",
    "stock_price['Date'] = pd.to_datetime(stock_price['Date']).dt.normalize()\n",
    "\n",
    "# filtering the important columns required\n",
    "stock_price = stock_price.filter(['Date', 'Close', 'Open', 'High', 'Low', 'Volume'])\n",
    "\n",
    "# setting column 'Date' as the index column\n",
    "stock_price.set_index('Date', inplace= True)\n",
    "\n",
    "# sorting the data according to the index i.e 'Date'\n",
    "stock_price = stock_price.sort_index(ascending=True, axis=0)\n",
    "stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc32d8",
   "metadata": {},
   "source": [
    "### Stock News Headlines Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates\n",
    "stock_headlines = stock_headlines.drop_duplicates()\n",
    "\n",
    "# coverting the datatype of column 'Date' from type string to type 'datetime'\n",
    "stock_headlines['publish_date'] = stock_headlines['publish_date'].astype(str)\n",
    "stock_headlines['publish_date'] = stock_headlines['publish_date'].apply(lambda x: x[0:4]+'-'+x[4:6]+'-'+x[6:8])\n",
    "stock_headlines['publish_date'] = pd.to_datetime(stock_headlines['publish_date']).dt.normalize()\n",
    "\n",
    "# filtering the important columns required\n",
    "stock_headlines = stock_headlines.filter(['publish_date', 'headline_text'])\n",
    "\n",
    "# grouping the news headlines according to 'Date'\n",
    "stock_headlines = stock_headlines.groupby(['publish_date'])['headline_text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# setting column 'Date' as the index column\n",
    "stock_headlines.set_index('publish_date', inplace= True)\n",
    "\n",
    "# sorting the data according to the index i.e 'Date'\n",
    "stock_headlines = stock_headlines.sort_index(ascending=True, axis=0)\n",
    "stock_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594028a8",
   "metadata": {},
   "source": [
    "### Combined Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the datasets stock_price and stock_headlines\n",
    "stock_data = pd.concat([stock_price, stock_headlines], axis=1)\n",
    "\n",
    "# dropping the null values if any\n",
    "stock_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# displaying the combined stock_data\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea92ce0",
   "metadata": {},
   "source": [
    "### Calculating Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding empty sentiment columns to stock_data for later calculation\n",
    "stock_data['compound'] = ''\n",
    "stock_data['negative'] = ''\n",
    "stock_data['neutral'] = ''\n",
    "stock_data['positive'] = ''\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires libraries to analyze the sentiments\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "\n",
    "# instantiating the Sentiment Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# calculating sentiment scores\n",
    "stock_data['compound'] = stock_data['headline_text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "stock_data['negative'] = stock_data['headline_text'].apply(lambda x: sid.polarity_scores(x)['neg'])\n",
    "stock_data['neutral'] = stock_data['headline_text'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "stock_data['positive'] = stock_data['headline_text'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "\n",
    "# displaying the stock data\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the 'headline_text' which is unwanted now\n",
    "stock_data.drop(['headline_text'], inplace=True, axis=1)\n",
    "\n",
    "# rearranging the columns of the whole stock_data\n",
    "stock_data = stock_data[['Close', 'compound', 'negative', 'neutral', 'positive', 'Open', 'High', 'Low', 'Volume']]\n",
    "\n",
    "# displaying the final stock_data\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the prepared stock_data to disk\n",
    "stock_data.to_csv('/content/drive/MyDrive/TheSparkFoundation/stock_datafinal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f676da",
   "metadata": {},
   "source": [
    "###  Reading Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-reading the stock_data into pandas dataframe\n",
    "stock_data = pd.read_csv('/content/drive/MyDrive/TheSparkFoundation/stock_datafinal.csv', index_col = False)\n",
    "\n",
    "# renaming the column\n",
    "stock_data.rename(columns={'Unnamed: 0':'Date'}, inplace = True)\n",
    "\n",
    "# setting the column 'Date' as the index column\n",
    "stock_data.set_index('Date', inplace=True)\n",
    "\n",
    "# displaying the stock_data\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f672d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the shape i.e. number of rows and columns of stock_data\n",
    "stock_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "stock_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eda8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying stock_data information\n",
    "stock_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff031a1",
   "metadata": {},
   "source": [
    "###  EDA of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbdba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "# plotting close price\n",
    "stock_data['Close'].plot()\n",
    "\n",
    "# setting plot title, x and y labels\n",
    "plt.title(\"Close Price\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price ($)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating 7 day rolling mean\n",
    "stock_data.rolling(7).mean().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "# plotting the close price and a 30-day rolling mean of close price\n",
    "stock_data['Close'].plot()\n",
    "stock_data.rolling(window=30).mean()['Close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ebd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying stock_data\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7e1de",
   "metadata": {},
   "source": [
    "### Data Preparation for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating data_to_use\n",
    "percentage_of_data = 1.0\n",
    "data_to_use = int(percentage_of_data*(len(stock_data)-1))\n",
    "\n",
    "# using 80% of data for training\n",
    "train_end = int(data_to_use*0.8)\n",
    "total_data = len(stock_data)\n",
    "start = total_data - data_to_use\n",
    "\n",
    "# printing number of records in the training and test datasets\n",
    "print(\"Number of records in Training Data:\", train_end)\n",
    "print(\"Number of records in Test Data:\", total_data - train_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39829a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting one step ahead\n",
    "steps_to_predict = 1\n",
    "\n",
    "# capturing data to be used for each column\n",
    "close_price = stock_data.iloc[start:total_data,0] #close\n",
    "compound = stock_data.iloc[start:total_data,1] #compound\n",
    "negative = stock_data.iloc[start:total_data,2] #neg\n",
    "neutral = stock_data.iloc[start:total_data,3] #neu\n",
    "positive = stock_data.iloc[start:total_data,4] #pos\n",
    "open_price = stock_data.iloc[start:total_data,5] #open\n",
    "high = stock_data.iloc[start:total_data,6] #high\n",
    "low = stock_data.iloc[start:total_data,7] #low\n",
    "volume = stock_data.iloc[start:total_data,8] #volume\n",
    "\n",
    "# printing close price\n",
    "print(\"Close Price:\")\n",
    "close_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting next day close\n",
    "close_price_shifted = close_price.shift(-1) \n",
    "\n",
    "# shifting next day compound\n",
    "compound_shifted = compound.shift(-1) \n",
    "\n",
    "# concatenating the captured training data into a dataframe\n",
    "data = pd.concat([close_price, close_price_shifted, compound, compound_shifted, volume, open_price, high, low], axis=1)\n",
    "\n",
    "# setting column names of the revised stock data\n",
    "data.columns = ['close_price', 'close_price_shifted', 'compound', 'compound_shifted','volume', 'open_price', 'high', 'low']\n",
    "\n",
    "# dropping nulls\n",
    "data = data.dropna()    \n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the target variable as the shifted close_price\n",
    "y = data['close_price_shifted']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the features dataset for prediction  \n",
    "cols = ['close_price', 'compound', 'compound_shifted', 'volume', 'open_price', 'high', 'low']\n",
    "x = data[cols]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a2715",
   "metadata": {},
   "source": [
    "### Scaling the Target Variable and the Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f22550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the feature dataset\n",
    "scaler_x = preprocessing.MinMaxScaler (feature_range=(-1, 1))\n",
    "x = np.array(x).reshape((len(x) ,len(cols)))\n",
    "x = scaler_x.fit_transform(x)\n",
    "\n",
    "# scaling the target variable\n",
    "scaler_y = preprocessing.MinMaxScaler (feature_range=(-1, 1))\n",
    "y = np.array (y).reshape ((len( y), 1))\n",
    "y = scaler_y.fit_transform (y)\n",
    "\n",
    "# displaying the scaled feature dataset and the target variable\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add19b63",
   "metadata": {},
   "source": [
    "###  Dividing the dataset into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and test dataset\n",
    "X_train = x[0 : train_end,]\n",
    "X_test = x[train_end+1 : len(x),]    \n",
    "y_train = y[0 : train_end] \n",
    "y_test = y[train_end+1 : len(y)]  \n",
    "\n",
    "# printing the shape of the training and the test datasets\n",
    "print('Number of rows and columns in the Training set X:', X_train.shape, 'and y:', y_train.shape)\n",
    "print('Number of rows and columns in the Test set X:', X_test.shape, 'and y:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the feature dataset for feeding into the model\n",
    "X_train = X_train.reshape (X_train.shape + (1,)) \n",
    "X_test = X_test.reshape(X_test.shape + (1,))\n",
    "\n",
    "# printing the re-shaped feature dataset\n",
    "print('Shape of Training set X:', X_train.shape)\n",
    "print('Shape of Test set X:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the seed to achieve consistent and less random predictions at each execution\n",
    "np.random.seed(2016)\n",
    "\n",
    "# setting the model architecture\n",
    "model=Sequential()\n",
    "model.add(LSTM(100,return_sequences=True,activation='tanh',input_shape=(len(cols),1)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(100,return_sequences=True,activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(100,activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# printing the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='mse' , optimizer='adam')\n",
    "\n",
    "# fitting the model using the training dataset\n",
    "model.fit(X_train, y_train, validation_split=0.35, epochs=50, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c57b8b",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing predictions\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "# unscaling the predictions\n",
    "predictions = scaler_y.inverse_transform(np.array(predictions).reshape((len(predictions), 1)))\n",
    "\n",
    "# printing the predictions\n",
    "print('Predictions:')\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d161540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the training mean-squared-error\n",
    "train_loss = model.evaluate(X_train, y_train, batch_size = 1)\n",
    "\n",
    "# calculating the test mean-squared-error\n",
    "test_loss = model.evaluate(X_test, y_test, batch_size = 1)\n",
    "\n",
    "# printing the training and the test mean-squared-errors\n",
    "print('Train Loss =', round(train_loss,4))\n",
    "print('Test Loss =', round(test_loss,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating root mean squared error\n",
    "root_mean_square_error = np.sqrt(np.mean(np.power((y_test - predictions),2)))\n",
    "print('Root Mean Square Error =', round(root_mean_square_error,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48711b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating root mean squared error using sklearn.metrics package\n",
    "rmse = metrics.mean_squared_error(y_test, predictions)\n",
    "print('Root Mean Square Error (sklearn.metrics) =', round(np.sqrt(rmse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscaling the test feature dataset, x_test\n",
    "X_test = scaler_x.inverse_transform(np.array(X_test).reshape((len(X_test), len(cols))))\n",
    "\n",
    "# unscaling the test y dataset, y_test\n",
    "y_train = scaler_y.inverse_transform(np.array(y_train).reshape((len(y_train), 1)))\n",
    "y_test = scaler_y.inverse_transform(np.array(y_test).reshape((len(y_test), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76054c5",
   "metadata": {},
   "source": [
    "### Plotting the Predictions against unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "# plt.plot([row[0] for row in y_train], label=\"Training Close Price\")\n",
    "plt.plot(predictions, label=\"Predicted Close Price\")\n",
    "plt.plot([row[0] for row in y_test], label=\"Testing Close Price\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252bec0",
   "metadata": {},
   "source": [
    "# Thank You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582541ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
